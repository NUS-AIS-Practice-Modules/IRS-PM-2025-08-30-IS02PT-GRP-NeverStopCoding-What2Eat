{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ableyyyx/IRS-PM-2025-08-30-IS02PT-GRP-StopCoding-BrainBag/blob/LangChain/PDF-retrievers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf37a837-7a6a-447b-8779-38f26c585887",
      "metadata": {
        "id": "bf37a837-7a6a-447b-8779-38f26c585887"
      },
      "source": [
        "# Build a semantic search engine\n",
        "\n",
        "This tutorial will familiarize you with LangChain's [document loader](/docs/concepts/document_loaders), [embedding](/docs/concepts/embedding_models), and [vector store](/docs/concepts/vectorstores) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/docs/concepts/rag) (see our RAG tutorial [here](/docs/tutorials/rag)).\n",
        "\n",
        "Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\n",
        "\n",
        "## Concepts\n",
        "\n",
        "This guide focuses on retrieval of text data. We will cover the following concepts:\n",
        "\n",
        "- Documents and document loaders;\n",
        "- Text splitters;\n",
        "- Embeddings;\n",
        "- Vector stores and retrievers.\n",
        "\n",
        "## Setup\n",
        "\n",
        "### Jupyter Notebook\n",
        "\n",
        "This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.\n",
        "\n",
        "### Installation\n",
        "\n",
        "This tutorial requires the `langchain-community` and `pypdf` packages:\n",
        "\n",
        "import Tabs from '@theme/Tabs';\n",
        "import TabItem from '@theme/TabItem';\n",
        "import CodeBlock from \"@theme/CodeBlock\";\n",
        "\n",
        "<Tabs>\n",
        "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
        "    <CodeBlock language=\"bash\">pip install langchain-community pypdf</CodeBlock>\n",
        "  </TabItem>\n",
        "  <TabItem value=\"conda\" label=\"Conda\">\n",
        "    <CodeBlock language=\"bash\">conda install langchain-community pypdf -c conda-forge</CodeBlock>\n",
        "  </TabItem>\n",
        "</Tabs>\n",
        "\n",
        "\n",
        "For more details, see our [Installation guide](/docs/how_to/installation).\n",
        "\n",
        "### LangSmith\n",
        "\n",
        "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
        "As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
        "The best way to do this is with [LangSmith](https://smith.langchain.com).\n",
        "\n",
        "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
        "\n",
        "```shell\n",
        "export LANGSMITH_TRACING=\"true\"\n",
        "export LANGSMITH_API_KEY=\"...\"\n",
        "```\n",
        "\n",
        "Or, if in a notebook, you can set them with:\n",
        "\n",
        "```python\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
        "```\n",
        "\n",
        "\n",
        "## Documents and Document Loaders\n",
        "\n",
        "LangChain implements a [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
        "\n",
        "- `page_content`: a string representing the content;\n",
        "- `metadata`: a dict containing arbitrary metadata;\n",
        "- `id`: (optional) a string identifier for the document.\n",
        "\n",
        "The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual `Document` object often represents a chunk of a larger document.\n",
        "\n",
        "We can generate sample documents when desired:\n",
        "```python\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8593578-5699-4b19-96c4-7c990d37a2ec",
      "metadata": {
        "id": "f8593578-5699-4b19-96c4-7c990d37a2ec"
      },
      "source": [
        "However, the LangChain ecosystem implements [document loaders](/docs/concepts/document_loaders) that [integrate with hundreds of common sources](/docs/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.\n",
        "\n",
        "### Loading documents\n",
        "\n",
        "Let's load a PDF into a sequence of `Document` objects. There is a sample PDF in the LangChain repo [here](https://github.com/langchain-ai/langchain/tree/master/docs/docs/example_data) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/docs/integrations/document_loaders/#pdfs). Let's select [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/), which is fairly lightweight."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rqzx2ADqtOL",
        "outputId": "18bdbed8-354a-4e97-a06f-c5fbd9b36cb1"
      },
      "id": "2Rqzx2ADqtOL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<2.0.0,>=0.3.75 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.74\n",
            "    Uninstalling langchain-core-0.3.74:\n",
            "      Successfully uninstalled langchain-core-0.3.74\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.29 langchain-core-0.3.75 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-6.0.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndLDybpYrQm-",
        "outputId": "a18f1c4c-2e13-48e5-df22-6aa4619fcb93"
      },
      "id": "ndLDybpYrQm-",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from"
      ],
      "metadata": {
        "id": "96tYXdl4ruaH"
      },
      "id": "96tYXdl4ruaH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGF-WA7mr-rn",
        "outputId": "bc852c34-855c-426b-a6c1-b2fe67761e03"
      },
      "id": "DGF-WA7mr-rn",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a2ac32c4-1036-42d8-8a3d-f7f57e3a0df7",
      "metadata": {
        "id": "a2ac32c4-1036-42d8-8a3d-f7f57e3a0df7",
        "outputId": "8eb3dcf2-138c-45a0-bac7-4efa2542a95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"drive/MyDrive/nke-10k-2023.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90f4800-bb82-416b-beba-f42ae88a5c66",
      "metadata": {
        "id": "b90f4800-bb82-416b-beba-f42ae88a5c66"
      },
      "source": [
        ":::tip\n",
        "\n",
        "See [this guide](/docs/how_to/document_loader_pdf/) for more detail on PDF document loaders.\n",
        "\n",
        ":::\n",
        "\n",
        "`PyPDFLoader` loads one `Document` object per PDF page. For each, we can easily access:\n",
        "\n",
        "- The string content of the page;\n",
        "- Metadata containing the file name and page number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "850e2ca5-6b20-4e58-ad99-b19786358a3e",
      "metadata": {
        "id": "850e2ca5-6b20-4e58-ad99-b19786358a3e",
        "outputId": "86ce00c7-cd07-485c-e47d-8ac86f726325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table of Contents\n",
            "UNITED STATES\n",
            "SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "FORM 10-K\n",
            "(Mark One)\n",
            "☑  ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
            "F\n",
            "\n",
            "{'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': 'drive/MyDrive/nke-10k-2023.pdf', 'total_pages': 107, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "print(f\"{docs[0].page_content[:200]}\\n\")\n",
        "print(docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca6980f-4870-490a-9fe6-8caeead3c1d1",
      "metadata": {
        "id": "2ca6980f-4870-490a-9fe6-8caeead3c1d1"
      },
      "source": [
        "### Splitting\n",
        "\n",
        "For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve `Document` objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
        "\n",
        "We can use [text splitters](/docs/concepts/text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\n",
        "with 200 characters of overlap between chunks. The overlap helps\n",
        "mitigate the possibility of separating a statement from important\n",
        "context related to it. We use the\n",
        "[RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter),\n",
        "which will recursively split the document using common separators like\n",
        "new lines until each chunk is the appropriate size. This is the\n",
        "recommended text splitter for generic text use cases.\n",
        "\n",
        "We set `add_start_index=True` so that the character index where each\n",
        "split Document starts within the initial Document is preserved as\n",
        "metadata attribute “start_index”.\n",
        "\n",
        "See [this guide](/docs/how_to/document_loader_pdf/) for more detail about working with PDFs, including how to extract text from specific sections and images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "11c16e79-c8af-4949-9363-9a93a911a0e1",
      "metadata": {
        "id": "11c16e79-c8af-4949-9363-9a93a911a0e1",
        "outputId": "2aa20bfa-ea4c-43e8-8452-eb7fcc44e485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "1yOVIJgxt5zL",
        "outputId": "27a56f77-5cf6-4b2d-e345-766f8359e3d5"
      },
      "id": "1yOVIJgxt5zL",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Table of Contents\\nUNITED STATES\\nSECURITIES AND EXCHANGE COMMISSION\\nWashington, D.C. 20549\\nFORM 10-K\\n(Mark One)\\n☑  ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFOR THE FISCAL YEAR ENDED MAY 31, 2023\\nOR\\n☐  TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFOR THE TRANSITION PERIOD FROM                         TO                         .\\nCommission File No. 1-10635\\nNIKE, Inc.\\n(Exact name of Registrant as specified in its charter)\\nOregon 93-0584541\\n(State or other jurisdiction of incorporation) (IRS Employer Identification No.)\\nOne Bowerman Drive, Beaverton, Oregon 97005-6453\\n(Address of principal executive offices and zip code)\\n(503) 671-6453\\n(Registrant's telephone number, including area code)\\nSECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\\nClass B Common Stock NKE New York Stock Exchange\\n(Title of each class) (Trading symbol) (Name of each exchange on which registered)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[1].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ZNNy8iJpt7Ul",
        "outputId": "f0879ecd-ae70-413f-e32e-b5c04afc62e0"
      },
      "id": "ZNNy8iJpt7Ul",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\\nClass B Common Stock NKE New York Stock Exchange\\n(Title of each class) (Trading symbol) (Name of each exchange on which registered)\\nSECURITIES REGISTERED PURSUANT TO SECTION 12(G) OF THE ACT:\\nNONE\\nIndicate by check mark: YES NO\\n• if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act. þ ¨ \\n• if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act. ¨ þ \\n• whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding\\n12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the\\npast 90 days.\\nþ ¨ \\n• whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c066d46-9187-4d5a-98d3-974c37610276",
      "metadata": {
        "id": "5c066d46-9187-4d5a-98d3-974c37610276"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/docs/concepts/embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n",
        "\n",
        "LangChain supports embeddings from [dozens of providers](/docs/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:\n",
        "\n",
        "import EmbeddingTabs from \"@theme/EmbeddingTabs\";\n",
        "\n",
        "<EmbeddingTabs customVarName=\"embeddings\" />"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-qCV9tjtVR3",
        "outputId": "c9c7f57e-0fec-45ff-cf9c-ad6af51bbe5b"
      },
      "id": "M-qCV9tjtVR3",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4d238a38-b9b3-494a-9cea-8694a1b03bc7",
      "metadata": {
        "id": "4d238a38-b9b3-494a-9cea-8694a1b03bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d383989-3cab-4c98-de3e-0c9e6a11a85f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c5f3b0ac-4e18-4c6b-84e7-e8822c59ce17",
      "metadata": {
        "id": "c5f3b0ac-4e18-4c6b-84e7-e8822c59ce17",
        "outputId": "eb0e6ad2-1714-4096-f52c-20acf65abc86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated vectors of length 3072\n",
            "\n",
            "[0.009319962002336979, -0.015975290909409523, 0.00031626885174773633, 0.0064051286317408085, 0.02052893675863743, -0.03918137028813362, -0.007355889771133661, 0.04105787351727486, -0.008043941110372543, 0.05994800105690956]\n"
          ]
        }
      ],
      "source": [
        "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
        "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
        "\n",
        "assert len(vector_1) == len(vector_2)\n",
        "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
        "print(vector_1[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cac19bd-27d1-40f1-9c27-7a586b685b4e",
      "metadata": {
        "id": "1cac19bd-27d1-40f1-9c27-7a586b685b4e"
      },
      "source": [
        "Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n",
        "\n",
        "## Vector stores\n",
        "\n",
        "LangChain [VectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html) objects contain methods for adding text and `Document` objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/docs/how_to/embed_text) models, which determine how text data is translated to numeric vectors.\n",
        "\n",
        "LangChain includes a suite of [integrations](/docs/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/docs/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:\n",
        "\n",
        "import VectorStoreTabs from \"@theme/VectorStoreTabs\";\n",
        "\n",
        "<VectorStoreTabs/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-core langchain_chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAra4xV3uJ5_",
        "outputId": "d79ddda0-ba3a-40be-cfed-0047b26e2b7e"
      },
      "id": "wAra4xV3uJ5_",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5b0d3730-c008-4246-8b03-dd3058513e1c",
      "metadata": {
        "id": "5b0d3730-c008-4246-8b03-dd3058513e1c"
      },
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b3035f-1371-4965-ab7a-04eae25e47f3",
      "metadata": {
        "id": "e3b3035f-1371-4965-ab7a-04eae25e47f3"
      },
      "source": [
        "Having instantiated our vector store, we can now index the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2ea92e04-331c-4f83-aa2a-508322bdfbfc",
      "metadata": {
        "id": "2ea92e04-331c-4f83-aa2a-508322bdfbfc"
      },
      "outputs": [],
      "source": [
        "ids = vector_store.add_documents(documents=all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0f0b43-e5b8-4c79-b782-a02f17345487",
      "metadata": {
        "id": "ff0f0b43-e5b8-4c79-b782-a02f17345487"
      },
      "source": [
        "Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/docs/integrations/vectorstores) for more detail.\n",
        "\n",
        "Once we've instantiated a `VectorStore` that contains documents, we can query it. [VectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html) includes methods for querying:\n",
        "- Synchronously and asynchronously;\n",
        "- By string query and by vector;\n",
        "- With and without returning similarity scores;\n",
        "- By similarity and [maximum marginal relevance](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search) (to balance similarity with query to diversity in retrieved results).\n",
        "\n",
        "The methods will generally include a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects in their outputs.\n",
        "\n",
        "### Usage\n",
        "\n",
        "Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n",
        "\n",
        "Return documents based on similarity to a string query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7e01ed91-1a98-4221-960a-bd7a2541a548",
      "metadata": {
        "id": "7e01ed91-1a98-4221-960a-bd7a2541a548",
        "outputId": "2f03bf03-55df-429f-f980-c0149c102b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='operations. We also lease an office complex in Shanghai, China, our headquarters for our Greater China geography, occupied by employees focused on implementing our\n",
            "wholesale, NIKE Direct and merchandising strategies in the region, among other functions.\n",
            "In the United States, NIKE has eight significant distribution centers. Five are located in or near Memphis, Tennessee, two of which are owned and three of which are\n",
            "leased. Two other distribution centers, one located in Indianapolis, Indiana and one located in Dayton, Tennessee, are leased and operated by third-party logistics\n",
            "providers. One distribution center for Converse is located in Ontario, California, which is leased. NIKE has a number of distribution facilities outside the United States,\n",
            "some of which are leased and operated by third-party logistics providers. The most significant distribution facilities outside the United States are located in Laakdal,' metadata={'source': 'drive/MyDrive/nke-10k-2023.pdf', 'keywords': '0000320187-23-000039; ; 10-K', 'page': 26, 'creator': 'EDGAR Filing HTML Converter', 'moddate': '2023-07-20T16:22:08-04:00', 'page_label': '27', 'title': '0000320187-23-000039', 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creationdate': '2023-07-20T16:22:00-04:00', 'start_index': 804, 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'total_pages': 107, 'author': 'EDGAR Online, a division of Donnelley Financial Solutions'}\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"How many distribution centers does Nike have in the US?\"\n",
        ")\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4f9857-5a7d-4b5f-82b8-ff76539143c2",
      "metadata": {
        "id": "4d4f9857-5a7d-4b5f-82b8-ff76539143c2"
      },
      "source": [
        "Async query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7ff9e061-7710-40b2-93dc-1ca2b71ef96d",
      "metadata": {
        "id": "7ff9e061-7710-40b2-93dc-1ca2b71ef96d",
        "outputId": "aaa5e368-ec7a-4ec1-f07c-ee559411414d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Table of Contents\n",
            "PART I\n",
            "ITEM 1. BUSINESS\n",
            "GENERAL\n",
            "NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n",
            "\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\n",
            "Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\n",
            "the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\n",
            "and sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'title': '0000320187-23-000039', 'page_label': '4', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'total_pages': 107, 'source': 'drive/MyDrive/nke-10k-2023.pdf', 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'start_index': 0, 'creationdate': '2023-07-20T16:22:00-04:00', 'keywords': '0000320187-23-000039; ; 10-K', 'creator': 'EDGAR Filing HTML Converter', 'page': 3, 'moddate': '2023-07-20T16:22:08-04:00'}\n"
          ]
        }
      ],
      "source": [
        "results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4172698-9ad7-4422-99b2-bdc268e99c75",
      "metadata": {
        "id": "d4172698-9ad7-4422-99b2-bdc268e99c75"
      },
      "source": [
        "Return scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "52dfc576-40a7-4030-aeb5-bb4d3a493e3e",
      "metadata": {
        "id": "52dfc576-40a7-4030-aeb5-bb4d3a493e3e",
        "outputId": "9e7e2ec2-bb18-45ea-d0e6-9cd62756a99f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.6236849427223206\n",
            "\n",
            "page_content='Table of Contents\n",
            "FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\n",
            "FISCAL 2023 COMPARED TO FISCAL 2022\n",
            "• NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\n",
            "The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n",
            "2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n",
            "• NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\n",
            "increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\n",
            "equivalent basis.' metadata={'total_pages': 107, 'page_label': '36', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'page': 35, 'moddate': '2023-07-20T16:22:08-04:00', 'creator': 'EDGAR Filing HTML Converter', 'source': 'drive/MyDrive/nke-10k-2023.pdf', 'start_index': 0, 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'keywords': '0000320187-23-000039; ; 10-K', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31'}\n"
          ]
        }
      ],
      "source": [
        "# Note that providers implement different scores; the score here\n",
        "# is a distance metric that varies inversely with similarity.\n",
        "\n",
        "results = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\n",
        "doc, score = results[0]\n",
        "print(f\"Score: {score}\\n\")\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4991642-7275-40a9-b11a-e3beccbf2614",
      "metadata": {
        "id": "b4991642-7275-40a9-b11a-e3beccbf2614"
      },
      "source": [
        "Return documents based on similarity to an embedded query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7be726c1-b24c-414a-9862-d412b94784b2",
      "metadata": {
        "id": "7be726c1-b24c-414a-9862-d412b94784b2",
        "outputId": "baae8d40-3b7b-4d79-d55a-cffe1d2516ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Table of Contents\n",
            "GROSS MARGIN\n",
            "FISCAL 2023 COMPARED TO FISCAL 2022\n",
            "For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n",
            "43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n",
            "*Wholesale equivalent\n",
            "The decrease in gross margin for fiscal 2023 was primarily due to:\n",
            "• Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\n",
            "product mix;\n",
            "• Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\n",
            "the prior period resulting from lower available inventory supply;\n",
            "• Unfavorable changes in net foreign currency exchange rates, including hedges; and\n",
            "• Lower off-price margin, on a wholesale equivalent basis.\n",
            "This was partially offset by:' metadata={'page': 36, 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'keywords': '0000320187-23-000039; ; 10-K', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'title': '0000320187-23-000039', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'page_label': '37', 'source': 'drive/MyDrive/nke-10k-2023.pdf', 'moddate': '2023-07-20T16:22:08-04:00', 'creator': 'EDGAR Filing HTML Converter', 'start_index': 0, 'creationdate': '2023-07-20T16:22:00-04:00', 'total_pages': 107}\n"
          ]
        }
      ],
      "source": [
        "embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n",
        "\n",
        "results = vector_store.similarity_search_by_vector(embedding)\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168dbbec-ea97-4cc9-bb1a-75519c2d08af",
      "metadata": {
        "id": "168dbbec-ea97-4cc9-bb1a-75519c2d08af"
      },
      "source": [
        "Learn more:\n",
        "\n",
        "- [API reference](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html)\n",
        "- [How-to guide](/docs/how_to/vectorstores)\n",
        "- [Integration-specific docs](/docs/integrations/vectorstores)\n",
        "\n",
        "## Retrievers\n",
        "\n",
        "LangChain `VectorStore` objects do not subclass [Runnable](https://python.langchain.com/api_reference/core/index.html#langchain-core-runnables). LangChain [Retrievers](https://python.langchain.com/api_reference/core/index.html#langchain-core-retrievers) are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\n",
        "\n",
        "We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "58b8e826-1556-489c-b27b-a1efbc4cd689",
      "metadata": {
        "id": "58b8e826-1556-489c-b27b-a1efbc4cd689",
        "outputId": "0b2866b5-49ee-4468-96e6-08bfad4313f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='24b58822-1871-41d0-a623-af494f3dce28', metadata={'start_index': 804, 'source': 'drive/MyDrive/nke-10k-2023.pdf', 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creationdate': '2023-07-20T16:22:00-04:00', 'moddate': '2023-07-20T16:22:08-04:00', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'title': '0000320187-23-000039', 'page': 26, 'creator': 'EDGAR Filing HTML Converter', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'total_pages': 107, 'keywords': '0000320187-23-000039; ; 10-K', 'page_label': '27'}, page_content='operations. We also lease an office complex in Shanghai, China, our headquarters for our Greater China geography, occupied by employees focused on implementing our\\nwholesale, NIKE Direct and merchandising strategies in the region, among other functions.\\nIn the United States, NIKE has eight significant distribution centers. Five are located in or near Memphis, Tennessee, two of which are owned and three of which are\\nleased. Two other distribution centers, one located in Indianapolis, Indiana and one located in Dayton, Tennessee, are leased and operated by third-party logistics\\nproviders. One distribution center for Converse is located in Ontario, California, which is leased. NIKE has a number of distribution facilities outside the United States,\\nsome of which are leased and operated by third-party logistics providers. The most significant distribution facilities outside the United States are located in Laakdal,')],\n",
              " [Document(id='33c7b15a-b9e9-4943-a168-a925343988ba', metadata={'source': 'drive/MyDrive/nke-10k-2023.pdf', 'title': '0000320187-23-000039', 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'page_label': '4', 'keywords': '0000320187-23-000039; ; 10-K', 'page': 3, 'start_index': 0, 'creationdate': '2023-07-20T16:22:00-04:00', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'total_pages': 107, 'moddate': '2023-07-20T16:22:08-04:00', 'creator': 'EDGAR Filing HTML Converter'}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "def retriever(query: str) -> List[Document]:\n",
        "    return vector_store.similarity_search(query, k=1)\n",
        "\n",
        "\n",
        "retriever.batch(\n",
        "    [\n",
        "        \"How many distribution centers does Nike have in the US?\",\n",
        "        \"When was Nike incorporated?\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36d3f64-a8bc-4baa-b2ea-07e324a0143e",
      "metadata": {
        "id": "a36d3f64-a8bc-4baa-b2ea-07e324a0143e"
      },
      "source": [
        "Vectorstores implement an `as_retriever` method that will generate a Retriever, specifically a [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html). These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4989fe5e-ac58-4751-bc35-f53ff885860c",
      "metadata": {
        "id": "4989fe5e-ac58-4751-bc35-f53ff885860c",
        "outputId": "a07344b1-04f9-47b4-954d-ec51b47dfd2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='24b58822-1871-41d0-a623-af494f3dce28', metadata={'start_index': 804, 'creationdate': '2023-07-20T16:22:00-04:00', 'creator': 'EDGAR Filing HTML Converter', 'page_label': '27', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'title': '0000320187-23-000039', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'total_pages': 107, 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'page': 26, 'source': 'drive/MyDrive/nke-10k-2023.pdf'}, page_content='operations. We also lease an office complex in Shanghai, China, our headquarters for our Greater China geography, occupied by employees focused on implementing our\\nwholesale, NIKE Direct and merchandising strategies in the region, among other functions.\\nIn the United States, NIKE has eight significant distribution centers. Five are located in or near Memphis, Tennessee, two of which are owned and three of which are\\nleased. Two other distribution centers, one located in Indianapolis, Indiana and one located in Dayton, Tennessee, are leased and operated by third-party logistics\\nproviders. One distribution center for Converse is located in Ontario, California, which is leased. NIKE has a number of distribution facilities outside the United States,\\nsome of which are leased and operated by third-party logistics providers. The most significant distribution facilities outside the United States are located in Laakdal,')],\n",
              " [Document(id='33c7b15a-b9e9-4943-a168-a925343988ba', metadata={'total_pages': 107, 'start_index': 0, 'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'creationdate': '2023-07-20T16:22:00-04:00', 'page': 3, 'moddate': '2023-07-20T16:22:08-04:00', 'keywords': '0000320187-23-000039; ; 10-K', 'source': 'drive/MyDrive/nke-10k-2023.pdf', 'title': '0000320187-23-000039', 'creator': 'EDGAR Filing HTML Converter', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'page_label': '4'}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "\n",
        "retriever.batch(\n",
        "    [\n",
        "        \"How many distribution centers does Nike have in the US?\",\n",
        "        \"When was Nike incorporated?\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b79ded3-39ed-4aeb-8b70-cd36795ae239",
      "metadata": {
        "id": "6b79ded3-39ed-4aeb-8b70-cd36795ae239"
      },
      "source": [
        "`VectorStoreRetriever` supports search types of `\"similarity\"` (default), `\"mmr\"` (maximum marginal relevance, described above), and `\"similarity_score_threshold\"`. We can use the latter to threshold documents output by the retriever by similarity score.\n",
        "\n",
        "Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/docs/concepts/rag) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/docs/tutorials/rag) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9be7cb-2081-48a4-b6e4-d5e2d562ffd4",
      "metadata": {
        "id": "3d9be7cb-2081-48a4-b6e4-d5e2d562ffd4"
      },
      "source": [
        "### Learn more:\n",
        "\n",
        "Retrieval strategies can be rich and complex. For example:\n",
        "\n",
        "- We can [infer hard rules and filters](/docs/how_to/self_query/) from a query (e.g., \"using documents published after 2020\");\n",
        "- We can [return documents that are linked](/docs/how_to/parent_document_retriever/) to the retrieved context in some way (e.g., via some document taxonomy);\n",
        "- We can generate [multiple embeddings](/docs/how_to/multi_vector) for each unit of context;\n",
        "- We can [ensemble results](/docs/how_to/ensemble_retriever) from multiple retrievers;\n",
        "- We can assign weights to documents, e.g., to weigh [recent documents](/docs/how_to/time_weighted_vectorstore/) higher.\n",
        "\n",
        "The [retrievers](/docs/how_to#retrievers) section of the how-to guides covers these and other built-in retrieval strategies.\n",
        "\n",
        "It is also straightforward to extend the [BaseRetriever](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html) class in order to implement custom retrievers. See our how-to guide [here](/docs/how_to/custom_retriever).\n",
        "\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You've now seen how to build a semantic search engine over a PDF document.\n",
        "\n",
        "For more on document loaders:\n",
        "\n",
        "- [Conceptual guide](/docs/concepts/document_loaders)\n",
        "- [How-to guides](/docs/how_to/#document-loaders)\n",
        "- [Available integrations](/docs/integrations/document_loaders/)\n",
        "\n",
        "For more on embeddings:\n",
        "\n",
        "- [Conceptual guide](/docs/concepts/embedding_models/)\n",
        "- [How-to guides](/docs/how_to/#embedding-models)\n",
        "- [Available integrations](/docs/integrations/text_embedding/)\n",
        "\n",
        "For more on vector stores:\n",
        "\n",
        "- [Conceptual guide](/docs/concepts/vectorstores/)\n",
        "- [How-to guides](/docs/how_to/#vector-stores)\n",
        "- [Available integrations](/docs/integrations/vectorstores/)\n",
        "\n",
        "For more on RAG, see:\n",
        "\n",
        "- [Build a Retrieval Augmented Generation (RAG) App](/docs/tutorials/rag/)\n",
        "- [Related how-to guides](/docs/how_to/#qa-with-rag)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}